# Basic Concepts

# Basic Concepts

## HPC Architecture

High-Performance Computing (HPC) systems are designed to process large-scale computational workloads by distributing tasks across multiple machines. These systems typically consist of a **computing cluster**, which is a network of interconnected computers called **nodes**.

HPC clusters have different types of nodes, each serving a specific function:

- **Login Nodes**
  - When you access the cluster, you are placed on a **login node**.
  - These nodes are meant for **code development, job submission, and lightweight tasks** (e.g., editing scripts, compiling programs, data transfer).

- **Compute Nodes**
  - These are the nodes where actual computations take place.
  - Jobs are **submitted** to compute nodes via a **job scheduler** (explained below).

### Job Scheduling and Resource Management

Because many users share an HPC system, a **scheduler** manages access to compute resources.

- **Schedulers** allocate jobs based on resource requests (CPU, memory, time, etc.) and **queue them** based on priority, availability, and system policies.
- **SLURM** is the job scheduler used in MODI. It handles **job submission, queuing, execution, and monitoring**.
- When submitting a job, you specify the required **compute resources** (number of CPUs, amount of memory, time limit, etc.). The scheduler then assigns a compute node based on availability and priority.

> **Important:**  
> Submitting jobs with excessive resource requests can lead to **longer wait times**. Efficient resource requests lead to better scheduling for all users.

### Partitions and Job Priorities

Nodes in an HPC cluster are grouped into **partitions** (sometimes called queues).

- Partitions help to organize resources based on different job types.
- Jobs in partitions with **higher priority** will run before jobs in lower-priority partitions when resources are limited.

## MODI's Architecture

The **MODI HPC cluster** consists of **8 compute nodes**, each equipped with: **32 CPU cores** (64 threads) and **256 GB of RAM**

### MODI’s Compute Partitions

MODI uses partitions to allocate computing resources based on workload type and priority.

| **Partition**          | **Nodes**        | **Priority** | **Max Wall Time**       | **Use Case**                                      |
|------------------------|------------------|--------------|-------------------------|---------------------------------------------------|
| `modi_HPPC`            | `n[000-007]` (8) | 20           | `00:05:00` (5 min)      | High-priority short tests (used for HPC training) |
| `modi_devel` (default) | `n[002-007]` (6) | 10           | `00:15:00` (15 min)     | Short debugging & development runs                |
| `modi_short`           | `n[002-007]` (6) | 5            | `2-00:00:00` (2 days)   | Standard short-duration jobs                      |
| `modi_long`            | `n[002-007]` (6) | 3            | `7-00:00:00` (1 week)   | Longer-running jobs                               |
| `modi_max`             | `n[002-007]` (6) | 1            | `31-00:00:00` (1 month) | Extended computations                             |

- **Higher priority** means a job will be scheduled ahead of lower-priority jobs when resources are limited.
- The **`modi_HPPC` partition** has the highest priority but a **maximum wall time of 5-minutes**, I'm pretty sure this partition is just used for HPC training cources.
- The **default partition (`modi_devel`)** allows up to **15 minutes** and is best for debugging.
- **The longer the wall time, the lower the priority.** Jobs in `modi_long` and `modi_max` may experience longer queue times when the cluster is busy.
- Request only the resources you need. Large requests (e.g., entire nodes or long runtimes) may delay job execution.

> [!TIP]
> For more information about partitions, refer to the [User Guide – Part 5](https://oidc.erda.dk/public/MODI-user-guide.pdf#4.3=&page=4.53)

## MODI's Special Directories

MODI has some important default directories to take note of.

- `erda mount` is where your peronal ERDA home is mounted. The I/O rate is limited by the bandwidth available
between the MODI and ERDA systems, at any point in time the access speed can fluctuate depending on the current usage.

- `modi_mount` is mounted on every compute node. Any file that an individual node needs to have access to as part of job execution, needs to be located in this directory. Any output generated by a job also needs to be placed in this directory or it won’t be retrievable upon job completion. **`modi_mount` is limited to a maximum of 50 GB per user. Any data that is written beyond this limit will be refused with a ”Disk quota exceeded” return message.** Data not currently in use can be moved to `erda_mount` for storage and retrieval.

- `modi_images` contains pre-built Singularity images for use when executing SLURM jobs that require special libraries that are not preinstalled on the compute nodes (for example, `conda`). This is [discussed further below](#slurm)).

> [!TIP]
> For more information about MODI directories, refer to the [User Guide – Part 4.2](https://oidc.erda.dk/public/MODI-user-guide.pdf#4.3=&page=4.29)

## SLURM

...