# Basic Concepts

## HPC Architecture

A computing cluster is a network of computers called *nodes*.

When you log into the cluster, you are dropped into the *login* node. The real work on a cluster gets done by separate *compute* nodes.

Interaction between the *login* node and the *compute* nodes is handled by special software called a scheduler (SLURM is a type of scheduler, [discussed further below](#slurm)).


Nodes are allocated to *partitions*, which group them into logical sets. These can be overlapping, but may also for example, separate CPU focused nodes from GPU focused nodes.

## MODI's Architecture

MODI has 8 *compute* nodes, each with 32 CPUs (64 threads) and 256GB memory. 

> [!NOTE]
> Two of these nodes `n[000-001]` are only allocated to the partition `modi_HPPC`, this partition is also linked to all other nodes and has the highest priority, but has a max walltime of only 5 minutes. I think this partition is specifically set up for HPC training courses, and is probably otherwise not overly useful.

The remaining 6 nodes `n[002-007]` are allocated to four other partitions.

| partition              | nodes             | priority | max time                |
|------------------------|-------------------|----------|-------------------------|
| `modi_HPPC`            | `n\[000-007]` (8) | 20       | `   00:05:00`           |
| `modi_devel` (default) | `n\[002-007]` (6) | 10       | `   00:15:00`           |
| `modi_short`           | `n\[002-007]` (6) | 5        | ` 2-00:00:00` (2 days)  |
| `modi_long`            | `n\[002-007]` (6) | 3        | ` 7-00:00:00` (1 week)  |
| `modi_max`             | `n\[002-007]` (6) | 1        | `31-00:00:00` (1 month) |

Since the partitions handle overlapping nodes, they can be (and are) used as a means of assigning priority to jobs, based on their maximum walltime.

When all resources are in use, jobs with a higher priority will be placed in the queue ahead of those with a lower priority. I.e., if you can, request less resources to get your jobs running sooner. This only really matters when the cluster is busy, but it's also good practice (and ettiquette) to only request the resources that you need.

> [!TIP]
> For more information, refer to the [User Guide – 5](https://oidc.erda.dk/public/MODI-user-guide.pdf#4.3=&page=4.53)

## MODI's Special Directories

MODI has some important default directories to take note of.

- `erda mount` is where your peronal ERDA home is mounted. The I/O rate is limited by the bandwidth available
between the MODI and ERDA systems, at any point in time the access speed can fluctuate depending on the current usage.

- `modi_mount` is mounted on every compute node. Any file that an individual node needs to have access to as part of job execution, needs to be located in this directory. Any output generated by a job also needs to be placed in this directory or it won’t be retrievable upon job completion. **`modi_mount` is limited to a maximum of 50 GB per user. Any data that is written beyond this limit will be refused with a ”Disk quota exceeded” return message.** Data not currently in use can be moved to `erda_mount` for storage and retrieval.

- `modi_images` contains pre-built Singularity images for use when executing SLURM jobs that require special libraries that are not preinstalled on the compute nodes (for example, `conda`). This is explained further below.

> [!TIP]
> For more information, refer to the [User Guide – 4.2](https://oidc.erda.dk/public/MODI-user-guide.pdf#4.3=&page=4.29)

## SLURM

